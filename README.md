# README

This README would normally document whatever steps are necessary to get the
application up and running.

Things you may want to cover:

* Ruby version

* System dependencies

* Configuration

* Database creation

* Database initialization

* How to run the test suite

* Services (job queues, cache servers, search engines, etc.)

* Deployment instructions

* ...

## Code Review Rubric

Our code review process evaluates submissions across several key areas, with a focus on clarity, simplicity, and maintainable code. The total possible score is 130 points.

### Code Clarity (45 points)
We value readable, well-organized code that is easy to maintain.

| Category | Points | What We Look For |
|----------|--------|------------------|
| Naming Conventions | 15 | - Clear, descriptive names for variables, methods, and classes<br>- Consistent naming patterns<br>- Language-appropriate conventions |
| Method Simplicity | 15 | - Short, focused methods<br>- Single responsibility principle<br>- Clear purpose and intent |
| Code Organization | 10 | - Logical file/folder structure<br>- Related code grouped together<br>- Easy to navigate |
| Comments Quality | 5 | - Helpful comments explaining "why" not "what"<br>- Clear documentation where needed<br>- Not excessive or redundant |

### Architectural Choices (15 points)
We value good separation of concerns and appropriate use of framework features.

| Category | Points | What We Look For |
|----------|--------|------------------|
| Separation of Concerns | 5 | - Business logic in appropriate places<br>- Fat models, skinny controllers<br>- Clear responsibilities |
| File Organization | 5 | - Follows framework conventions<br>- Consistent structure<br>- Logical grouping |
| Framework Usage | 5 | - Appropriate use of framework features<br>- Following framework conventions<br>- Not fighting the framework |

### Development Practices (30 points)
We value good development practices that make code easier to maintain and collaborate on.

| Category | Points | What We Look For |
|----------|--------|------------------|
| Commit Quality | 15 | - Small, focused commits<br>- Clear commit messages<br>- Logical commit history |
| Documentation | 10 | - Clear README<br>- Setup instructions<br>- Key decisions documented |
| Code Standards | 5 | - Consistent formatting<br>- Language conventions<br>- Best practices followed |

### Problem Solving (25 points)
We value simple, effective solutions over clever complexity.

| Category | Points | What We Look For |
|----------|--------|------------------|
| Solution Simplicity | 15 | - Straightforward approaches<br>- Avoiding unnecessary complexity<br>- Clear problem-solving logic |
| Code Reuse | 10 | - DRY without over-abstraction<br>- Appropriate shared code<br>- Balance of reuse vs. simplicity |

### Bonus Points (15 points)
Additional points for testing practices.

| Category | Points | What We Look For |
|----------|--------|------------------|
| Basic Testing | 8 | - Framework tests present<br>- Basic unit tests<br>- Key functionality tested |
| Test Coverage | 4 | - Core functionality coverage<br>- Multiple test cases<br>- Edge case handling |
| Test Organization | 3 | - Well-structured test files<br>- Test helpers and shared examples<br>- Clear test patterns |

## Notes
- This rubric is language-agnostic and applies to all submissions
- We prefer simple, maintainable solutions over clever, complex ones
- Basic testing is valued and will earn bonus points
- Small, focused commits are highly valued for easier maintenance

## Review Process

### How Reviews Are Conducted

#### 1. Repository Submission
- Submit a GitHub repository URL through our review platform
- Repository should be public or access granted to the reviewer
- Main/Master branch will be reviewed unless otherwise specified

#### 2. Initial Analysis
Our system performs several automated checks:
- Clones the repository
- Identifies primary language(s) and frameworks
- Analyzes commit history
- Scans for basic structure (tests, documentation, etc.)

#### 3. Code Analysis
The review combines automated and manual analysis:

**Automated Checks:**
- Code complexity metrics
- Style guide compliance
- Common anti-patterns
- Security vulnerabilities
- Test coverage (if tests present)

**Manual Review:**
- Architecture evaluation
- Code organization assessment
- Problem-solving approach
- Documentation quality
- Commit history review

#### 4. Scoring
- Each category is scored according to the rubric above
- Both automated metrics and manual review inform the final scores
- Bonus points are awarded for exceptional practices
- Non-working solutions may receive point deductions

#### 5. Results and Feedback
The review produces:
- Detailed scores for each category
- Specific examples supporting the scores
- Constructive feedback for improvement
- Highlighted areas of excellence
- Suggestions for next steps

### Review Standards
- Reviews are conducted against industry best practices
- Language-specific conventions are considered
- Framework-specific patterns are evaluated when applicable
- Simple, maintainable solutions are preferred over complex ones

### Timeline
- Initial automated analysis: Immediate
- Complete review: Usually within 24-48 hours
- Results: Available immediately after review completion
